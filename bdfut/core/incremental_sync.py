"""
Sistema de Sincroniza√ß√£o Incremental
===================================

Sistema inteligente para sincroniza√ß√£o incremental de dados
com detec√ß√£o de mudan√ßas e otimiza√ß√µes avan√ßadas.
"""
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
import json

from .sportmonks_client import SportmonksClient
from .supabase_client import SupabaseClient
from .etl_metadata import ETLMetadataManager, ETLJobContext

logger = logging.getLogger(__name__)


class IncrementalSyncManager:
    """Gerenciador de sincroniza√ß√£o incremental"""
    
    # Configura√ß√µes de sincroniza√ß√£o por tipo de dados
    SYNC_STRATEGIES = {
        'fixtures_recent': {
            'window_days': 7,        # √öltimos 7 dias
            'future_days': 14,       # Pr√≥ximos 14 dias
            'frequency': 'hourly',   # A cada hora
            'priority': 'high'
        },
        'fixtures_today': {
            'window_days': 1,        # Apenas hoje
            'future_days': 1,        # Apenas amanh√£
            'frequency': 'every_15min', # A cada 15 minutos
            'priority': 'critical'
        },
        'teams_venues': {
            'frequency': 'daily',    # Uma vez por dia
            'priority': 'medium'
        },
        'standings': {
            'frequency': 'daily',    # Uma vez por dia
            'priority': 'medium'
        },
        'base_data': {
            'frequency': 'weekly',   # Uma vez por semana
            'priority': 'low'
        }
    }
    
    def __init__(self, use_redis: bool = True):
        """
        Inicializa o gerenciador de sincroniza√ß√£o incremental
        
        Args:
            use_redis: Usar cache Redis
        """
        self.sportmonks = SportmonksClient(
            enable_cache=True,
            use_redis=use_redis,
            cache_ttl_hours=4  # TTL menor para dados incrementais
        )
        self.supabase = SupabaseClient()
        self.metadata_manager = ETLMetadataManager()
        
        logger.info("‚úÖ IncrementalSyncManager inicializado")
    
    def get_last_sync_timestamp(self, sync_type: str) -> Optional[datetime]:
        """
        Obt√©m timestamp da √∫ltima sincroniza√ß√£o
        
        Args:
            sync_type: Tipo de sincroniza√ß√£o
            
        Returns:
            Timestamp da √∫ltima sincroniza√ß√£o ou None
        """
        try:
            # Buscar √∫ltimo job bem-sucedido deste tipo
            recent_jobs = self.metadata_manager.get_recent_jobs(limit=50, job_type='fixtures_events')
            
            for job in recent_jobs:
                if (job.get('job_name', '').startswith(f'incremental_sync_{sync_type}') and 
                    job.get('status') == 'completed'):
                    return datetime.fromisoformat(job['completed_at'].replace('Z', '+00:00'))
            
            return None
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao obter timestamp da √∫ltima sincroniza√ß√£o: {e}")
            return None
    
    def detect_changes(self, sync_type: str) -> Dict[str, Any]:
        """
        Detecta mudan√ßas desde a √∫ltima sincroniza√ß√£o
        
        Args:
            sync_type: Tipo de sincroniza√ß√£o
            
        Returns:
            Informa√ß√µes sobre mudan√ßas detectadas
        """
        last_sync = self.get_last_sync_timestamp(sync_type)
        now = datetime.now()
        
        changes = {
            'last_sync': last_sync.isoformat() if last_sync else None,
            'current_time': now.isoformat(),
            'time_since_last_sync': None,
            'needs_sync': False,
            'sync_reason': '',
            'target_dates': []
        }
        
        if last_sync:
            time_since = now - last_sync.replace(tzinfo=None)
            changes['time_since_last_sync'] = int(time_since.total_seconds())
            
            strategy = self.SYNC_STRATEGIES.get(sync_type, {})
            frequency = strategy.get('frequency', 'daily')
            
            # Determinar se precisa sincronizar baseado na frequ√™ncia
            if frequency == 'every_15min' and time_since > timedelta(minutes=15):
                changes['needs_sync'] = True
                changes['sync_reason'] = 'Frequ√™ncia de 15 minutos atingida'
            elif frequency == 'hourly' and time_since > timedelta(hours=1):
                changes['needs_sync'] = True
                changes['sync_reason'] = 'Frequ√™ncia hor√°ria atingida'
            elif frequency == 'daily' and time_since > timedelta(days=1):
                changes['needs_sync'] = True
                changes['sync_reason'] = 'Frequ√™ncia di√°ria atingida'
            elif frequency == 'weekly' and time_since > timedelta(days=7):
                changes['needs_sync'] = True
                changes['sync_reason'] = 'Frequ√™ncia semanal atingida'
        else:
            changes['needs_sync'] = True
            changes['sync_reason'] = 'Primeira sincroniza√ß√£o'
        
        # Calcular datas alvo baseado na estrat√©gia
        if sync_type in self.SYNC_STRATEGIES:
            strategy = self.SYNC_STRATEGIES[sync_type]
            window_days = strategy.get('window_days', 7)
            future_days = strategy.get('future_days', 14)
            
            start_date = (now - timedelta(days=window_days)).strftime('%Y-%m-%d')
            end_date = (now + timedelta(days=future_days)).strftime('%Y-%m-%d')
            
            changes['target_dates'] = [start_date, end_date]
        
        return changes
    
    def sync_recent_fixtures(self, force: bool = False) -> Dict[str, Any]:
        """
        Sincroniza fixtures recentes e pr√≥ximas
        
        Args:
            force: For√ßar sincroniza√ß√£o mesmo se n√£o necess√°ria
            
        Returns:
            Estat√≠sticas da sincroniza√ß√£o
        """
        sync_type = 'fixtures_recent'
        
        with ETLJobContext(
            job_name=f"incremental_sync_{sync_type}",
            job_type="fixtures_events",
            metadata_manager=self.metadata_manager,
            script_path=__file__,
            input_parameters={"sync_type": sync_type, "force": force}
        ) as job:
            
            logger.info(f"üîÑ Sincroniza√ß√£o incremental: {sync_type}")
            job.log("INFO", f"Iniciando sincroniza√ß√£o incremental: {sync_type}")
            
            # Detectar mudan√ßas
            changes = self.detect_changes(sync_type)
            
            if not changes['needs_sync'] and not force:
                logger.info(f"‚è≠Ô∏è Sincroniza√ß√£o n√£o necess√°ria para {sync_type}")
                job.log("INFO", f"Sincroniza√ß√£o pulada - n√£o necess√°ria")
                return {
                    'sync_type': sync_type,
                    'skipped': True,
                    'reason': 'Sincroniza√ß√£o n√£o necess√°ria',
                    'last_sync': changes['last_sync']
                }
            
            logger.info(f"‚úÖ Sincroniza√ß√£o necess√°ria: {changes['sync_reason']}")
            job.log("INFO", f"Sincroniza√ß√£o necess√°ria: {changes['sync_reason']}")
            
            # Executar sincroniza√ß√£o
            start_date, end_date = changes['target_dates']
            
            logger.info(f"üìÖ Per√≠odo: {start_date} at√© {end_date}")
            job.log("INFO", f"Sincronizando per√≠odo: {start_date} at√© {end_date}")
            
            # Checkpoint inicial
            job.checkpoint(
                name="sync_started",
                data={
                    "sync_type": sync_type,
                    "start_date": start_date,
                    "end_date": end_date,
                    "changes_detected": changes
                },
                progress_percentage=10.0
            )
            
            try:
                # Buscar fixtures do per√≠odo
                fixtures = self.sportmonks.get_fixtures_by_date_range(
                    start_date=start_date,
                    end_date=end_date,
                    include='participants;state;venue;events'
                )
                
                job.increment_api_requests(len(fixtures) // 500 + 1)
                
                stats = {
                    'sync_type': sync_type,
                    'start_date': start_date,
                    'end_date': end_date,
                    'fixtures_found': len(fixtures),
                    'fixtures_processed': 0,
                    'fixtures_inserted': 0,
                    'fixtures_updated': 0,
                    'errors': 0,
                    'success': False
                }
                
                if fixtures:
                    logger.info(f"üìä {len(fixtures)} fixtures encontradas para sincroniza√ß√£o")
                    
                    # Checkpoint de fixtures encontradas
                    job.checkpoint(
                        name="fixtures_found",
                        data={
                            "fixtures_count": len(fixtures),
                            "next_step": "processing"
                        },
                        progress_percentage=30.0
                    )
                    
                    # Processar fixtures em batches
                    batch_size = 50
                    total_batches = (len(fixtures) + batch_size - 1) // batch_size
                    
                    for batch_idx in range(total_batches):
                        start_idx = batch_idx * batch_size
                        end_idx = min(start_idx + batch_size, len(fixtures))
                        batch = fixtures[start_idx:end_idx]
                        
                        try:
                            # Processar batch
                            batch_stats = self._process_fixtures_batch(batch, job)
                            
                            stats['fixtures_processed'] += batch_stats['processed']
                            stats['fixtures_inserted'] += batch_stats['inserted']
                            stats['fixtures_updated'] += batch_stats['updated']
                            stats['errors'] += batch_stats['errors']
                            
                            # Checkpoint de progresso
                            progress = ((batch_idx + 1) / total_batches) * 60 + 30  # 30-90%
                            job.checkpoint(
                                name=f"batch_{batch_idx + 1}_completed",
                                data={
                                    "batch_index": batch_idx + 1,
                                    "total_batches": total_batches,
                                    "stats_so_far": stats
                                },
                                progress_percentage=progress
                            )
                            
                            logger.info(f"‚úÖ Batch {batch_idx + 1}/{total_batches} processado")
                            
                        except Exception as e:
                            logger.error(f"‚ùå Erro no batch {batch_idx + 1}: {e}")
                            stats['errors'] += len(batch)
                            job.log("ERROR", f"Erro no batch {batch_idx + 1}: {e}")
                    
                    stats['success'] = stats['errors'] < (stats['fixtures_found'] * 0.1)  # < 10% erro
                    
                    logger.info(f"‚úÖ Sincroniza√ß√£o incremental conclu√≠da:")
                    logger.info(f"  üìä Fixtures processadas: {stats['fixtures_processed']}")
                    logger.info(f"  üìä Inseridas: {stats['fixtures_inserted']}")
                    logger.info(f"  üìä Atualizadas: {stats['fixtures_updated']}")
                    logger.info(f"  üìä Erros: {stats['errors']}")
                    
                    job.log("INFO", f"Sincroniza√ß√£o conclu√≠da - {stats['fixtures_processed']} fixtures processadas")
                    
                    # Checkpoint final
                    job.checkpoint(
                        name="sync_completed",
                        data=stats,
                        progress_percentage=100.0
                    )
                    
                else:
                    logger.info("üì≠ Nenhuma fixture encontrada para o per√≠odo")
                    stats['success'] = True
                
                return stats
                
            except Exception as e:
                logger.error(f"‚ùå Erro durante sincroniza√ß√£o: {e}")
                job.log("ERROR", f"Erro durante sincroniza√ß√£o: {e}")
                return {
                    'sync_type': sync_type,
                    'success': False,
                    'error': str(e)
                }
    
    def _process_fixtures_batch(self, fixtures: List[Dict], job: ETLJobContext) -> Dict[str, int]:
        """
        Processa um batch de fixtures
        
        Args:
            fixtures: Lista de fixtures
            job: Contexto do job
            
        Returns:
            Estat√≠sticas do processamento
        """
        batch_stats = {
            'processed': 0,
            'inserted': 0,
            'updated': 0,
            'errors': 0
        }
        
        try:
            # Salvar fixtures principais
            success = self.supabase.upsert_fixtures(fixtures)
            
            if success:
                batch_stats['processed'] = len(fixtures)
                batch_stats['updated'] = len(fixtures)  # Assumir update para incremental
                
                # Processar dados relacionados
                venues_data = []
                for fixture in fixtures:
                    # Venues
                    if 'venue' in fixture and fixture['venue']:
                        venues_data.append(fixture['venue'])
                    
                    # Participantes
                    if 'participants' in fixture and fixture['participants']:
                        self.supabase.upsert_fixture_participants(
                            fixture['id'], fixture['participants']
                        )
                    
                    # Eventos
                    if 'events' in fixture and fixture['events']:
                        self.supabase.upsert_fixture_events(
                            fixture['id'], fixture['events']
                        )
                
                # Salvar venues √∫nicos
                if venues_data:
                    # Remover duplicatas
                    unique_venues = {v['id']: v for v in venues_data if 'id' in v}.values()
                    self.supabase.upsert_venues(list(unique_venues))
                
                job.increment_records(
                    processed=batch_stats['processed'],
                    updated=batch_stats['updated']
                )
                
            else:
                batch_stats['errors'] = len(fixtures)
                job.increment_records(failed=batch_stats['errors'])
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao processar batch: {e}")
            batch_stats['errors'] = len(fixtures)
            job.increment_records(failed=batch_stats['errors'])
        
        return batch_stats
    
    def sync_today_fixtures(self, force: bool = False) -> Dict[str, Any]:
        """
        Sincroniza√ß√£o cr√≠tica de fixtures de hoje/amanh√£
        
        Args:
            force: For√ßar sincroniza√ß√£o
            
        Returns:
            Estat√≠sticas da sincroniza√ß√£o
        """
        return self.sync_recent_fixtures(force=force)  # Usar a mesma l√≥gica otimizada
    
    def sync_team_standings(self, season_ids: List[int] = None) -> Dict[str, Any]:
        """
        Sincroniza classifica√ß√µes das temporadas ativas
        
        Args:
            season_ids: IDs das temporadas (None = temporadas ativas)
            
        Returns:
            Estat√≠sticas da sincroniza√ß√£o
        """
        with ETLJobContext(
            job_name="incremental_sync_standings",
            job_type="leagues_seasons",
            metadata_manager=self.metadata_manager,
            input_parameters={"season_ids": season_ids}
        ) as job:
            
            logger.info("üìä Sincronizando classifica√ß√µes...")
            job.log("INFO", "Iniciando sincroniza√ß√£o de classifica√ß√µes")
            
            try:
                if season_ids is None:
                    # Buscar temporadas ativas
                    # Placeholder - implementar busca de temporadas ativas
                    season_ids = [23614, 21646]  # Exemplo
                
                total_standings = 0
                
                for season_id in season_ids:
                    logger.info(f"üìä Sincronizando classifica√ß√£o da temporada {season_id}")
                    
                    try:
                        standings = self.sportmonks.get_standings_by_season(season_id)
                        job.increment_api_requests(1)
                        
                        if standings:
                            # Aqui voc√™ implementaria o upsert de standings
                            # Por simplicidade, vou apenas contar
                            total_standings += len(standings)
                            job.increment_records(processed=len(standings), inserted=len(standings))
                            
                            logger.info(f"‚úÖ {len(standings)} entradas de classifica√ß√£o sincronizadas")
                        
                    except Exception as e:
                        logger.error(f"‚ùå Erro ao sincronizar classifica√ß√£o da temporada {season_id}: {e}")
                        job.log("ERROR", f"Erro na temporada {season_id}: {e}")
                
                logger.info(f"‚úÖ Sincroniza√ß√£o de classifica√ß√µes conclu√≠da: {total_standings} entradas")
                job.log("INFO", f"Classifica√ß√µes sincronizadas: {total_standings} entradas")
                
                return {
                    'sync_type': 'standings',
                    'seasons_processed': len(season_ids),
                    'standings_synced': total_standings,
                    'success': True
                }
                
            except Exception as e:
                logger.error(f"‚ùå Erro durante sincroniza√ß√£o de classifica√ß√µes: {e}")
                job.log("ERROR", f"Erro durante sincroniza√ß√£o: {e}")
                return {
                    'sync_type': 'standings',
                    'success': False,
                    'error': str(e)
                }
    
    def sync_base_data_incremental(self) -> Dict[str, Any]:
        """
        Sincroniza√ß√£o incremental de dados base
        
        Returns:
            Estat√≠sticas da sincroniza√ß√£o
        """
        with ETLJobContext(
            job_name="incremental_sync_base_data",
            job_type="base_data",
            metadata_manager=self.metadata_manager
        ) as job:
            
            logger.info("üîÑ Sincroniza√ß√£o incremental de dados base...")
            job.log("INFO", "Iniciando sincroniza√ß√£o incremental de dados base")
            
            try:
                stats = {
                    'countries_synced': 0,
                    'states_synced': 0,
                    'types_synced': 0,
                    'success': True
                }
                
                # Sincronizar countries
                countries = self.sportmonks.get_countries()
                job.increment_api_requests(1)
                if countries:
                    self.supabase.upsert_countries(countries)
                    stats['countries_synced'] = len(countries)
                    job.increment_records(processed=len(countries), updated=len(countries))
                
                # Sincronizar states
                states = self.sportmonks.get_states()
                job.increment_api_requests(1)
                if states:
                    self.supabase.upsert_states(states)
                    stats['states_synced'] = len(states)
                    job.increment_records(processed=len(states), updated=len(states))
                
                # Sincronizar types
                types = self.sportmonks.get_types()
                job.increment_api_requests(1)
                if types:
                    self.supabase.upsert_types(types)
                    stats['types_synced'] = len(types)
                    job.increment_records(processed=len(types), updated=len(types))
                
                logger.info(f"‚úÖ Dados base sincronizados:")
                logger.info(f"  üìä Countries: {stats['countries_synced']}")
                logger.info(f"  üìä States: {stats['states_synced']}")
                logger.info(f"  üìä Types: {stats['types_synced']}")
                
                job.log("INFO", f"Dados base sincronizados - Countries: {stats['countries_synced']}, States: {stats['states_synced']}, Types: {stats['types_synced']}")
                
                return stats
                
            except Exception as e:
                logger.error(f"‚ùå Erro durante sincroniza√ß√£o de dados base: {e}")
                job.log("ERROR", f"Erro durante sincroniza√ß√£o: {e}")
                return {
                    'success': False,
                    'error': str(e)
                }
    
    def run_daily_sync(self, include_standings: bool = True) -> Dict[str, Any]:
        """
        Executa sincroniza√ß√£o di√°ria completa
        
        Args:
            include_standings: Incluir sincroniza√ß√£o de classifica√ß√µes
            
        Returns:
            Estat√≠sticas completas da sincroniza√ß√£o
        """
        logger.info("üåÖ INICIANDO SINCRONIZA√á√ÉO DI√ÅRIA")
        logger.info("=" * 50)
        
        daily_stats = {
            'start_time': datetime.now(),
            'fixtures_sync': None,
            'standings_sync': None,
            'base_data_sync': None,
            'overall_success': False,
            'total_api_requests': 0,
            'total_records_processed': 0
        }
        
        try:
            # 1. Sincronizar fixtures recentes
            logger.info("1Ô∏è‚É£ Sincronizando fixtures recentes...")
            fixtures_result = self.sync_recent_fixtures()
            daily_stats['fixtures_sync'] = fixtures_result
            
            # 2. Sincronizar classifica√ß√µes (se solicitado)
            if include_standings:
                logger.info("2Ô∏è‚É£ Sincronizando classifica√ß√µes...")
                standings_result = self.sync_team_standings()
                daily_stats['standings_sync'] = standings_result
            
            # 3. Sincronizar dados base (se necess√°rio)
            base_data_changes = self.detect_changes('base_data')
            if base_data_changes['needs_sync']:
                logger.info("3Ô∏è‚É£ Sincronizando dados base...")
                base_data_result = self.sync_base_data_incremental()
                daily_stats['base_data_sync'] = base_data_result
            
            # Calcular sucesso geral
            daily_stats['overall_success'] = (
                daily_stats['fixtures_sync'].get('success', False) and
                (not include_standings or daily_stats['standings_sync'].get('success', False)) and
                (daily_stats['base_data_sync'] is None or daily_stats['base_data_sync'].get('success', False))
            )
            
            daily_stats['end_time'] = datetime.now()
            daily_stats['duration_seconds'] = int((daily_stats['end_time'] - daily_stats['start_time']).total_seconds())
            
            logger.info("=" * 50)
            logger.info(f"‚úÖ SINCRONIZA√á√ÉO DI√ÅRIA {'CONCLU√çDA' if daily_stats['overall_success'] else 'CONCLU√çDA COM PROBLEMAS'}")
            logger.info(f"‚è±Ô∏è Dura√ß√£o: {daily_stats['duration_seconds']}s")
            logger.info("=" * 50)
            
            return daily_stats
            
        except Exception as e:
            logger.error(f"‚ùå Erro durante sincroniza√ß√£o di√°ria: {e}")
            daily_stats['overall_success'] = False
            daily_stats['error'] = str(e)
            return daily_stats
    
    def get_sync_status(self) -> Dict[str, Any]:
        """
        Obt√©m status atual das sincroniza√ß√µes
        
        Returns:
            Status das sincroniza√ß√µes
        """
        status = {
            'last_sync_times': {},
            'next_sync_times': {},
            'sync_health': {}
        }
        
        for sync_type in self.SYNC_STRATEGIES.keys():
            last_sync = self.get_last_sync_timestamp(sync_type)
            changes = self.detect_changes(sync_type)
            
            status['last_sync_times'][sync_type] = last_sync.isoformat() if last_sync else None
            status['sync_health'][sync_type] = {
                'needs_sync': changes['needs_sync'],
                'reason': changes['sync_reason'],
                'time_since_last': changes['time_since_last_sync']
            }
        
        return status


class ScheduledSyncRunner:
    """Executor de sincroniza√ß√µes agendadas"""
    
    def __init__(self, sync_manager: IncrementalSyncManager):
        self.sync_manager = sync_manager
        
    def should_run_sync(self, sync_type: str) -> bool:
        """
        Verifica se deve executar sincroniza√ß√£o
        
        Args:
            sync_type: Tipo de sincroniza√ß√£o
            
        Returns:
            True se deve executar
        """
        changes = self.sync_manager.detect_changes(sync_type)
        return changes['needs_sync']
    
    def run_scheduled_syncs(self) -> Dict[str, Any]:
        """
        Executa sincroniza√ß√µes agendadas baseado na necessidade
        
        Returns:
            Resultados das sincroniza√ß√µes
        """
        logger.info("‚è∞ Verificando sincroniza√ß√µes agendadas...")
        
        results = {}
        
        # Verificar cada tipo de sincroniza√ß√£o
        for sync_type in self.sync_manager.SYNC_STRATEGIES.keys():
            if self.should_run_sync(sync_type):
                logger.info(f"üîÑ Executando sincroniza√ß√£o: {sync_type}")
                
                if sync_type == 'fixtures_recent':
                    results[sync_type] = self.sync_manager.sync_recent_fixtures()
                elif sync_type == 'fixtures_today':
                    results[sync_type] = self.sync_manager.sync_today_fixtures()
                elif sync_type == 'base_data':
                    results[sync_type] = self.sync_manager.sync_base_data_incremental()
                else:
                    logger.info(f"‚è≠Ô∏è Tipo de sincroniza√ß√£o {sync_type} n√£o implementado ainda")
            else:
                logger.info(f"‚è≠Ô∏è Sincroniza√ß√£o {sync_type} n√£o necess√°ria")
        
        return results
